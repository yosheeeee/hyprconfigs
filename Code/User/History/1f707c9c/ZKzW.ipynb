{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtisB7AuDRN5"
      },
      "source": [
        "# Экстраполяция временных рядов\n",
        "\n",
        "### Однопараметрическая экстраполяция\n",
        "\n",
        "![Забавный комикс](https://imgs.xkcd.com/comics/extrapolating.png)\n",
        "\n",
        "\n",
        "*Источник https://xkcd.com/605/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRGnY-UmDRN7"
      },
      "source": [
        "**Экстраполяция** — это процесс прогнозирования значений за пределами диапазона имеющихся данных. Например, если у вас есть данные за последние несколько лет, вы можете экстраполировать, как будут выглядеть значения в будущем. Это предполагает, что существующие тенденции будут продолжаться.\n",
        "\n",
        "**Задача регрессии** — это метод статистического анализа, который используется для предсказания значений зависимой переменной на основе одной или нескольких независимых переменных.\n",
        "*В этой задаче модель обучается на имеющихся данных, чтобы выявить зависимости и закономерности.*\n",
        "Основная цель регрессии заключается в том, чтобы установить зависимость между переменными и создать модель, которая может предсказывать значения зависимой переменной на основе новых данных. (Например, цены квартиры или телефона, курса доллара на завтра, ожидаемого объёма продаж, медицинских показателей до/после лечения и так далее)\n",
        "\n",
        "Например, пусть X = ξ+η – это сумма случайных величин ξ и η, а Y = ξ + ϕ – сумма случайных величин ξ и ϕ. Ясно, что величины X и Y зависимы, но нет явной функциональной зависимости, то есть мы не можем указать зависимость вида X = f(Y) или Y = f(X).\n",
        "\n",
        "Таким образом, основное различие в том, что регрессия используется для моделирования зависимости на основе имеющихся данных, а экстраполяция — для предсказания значений вне диапазона этих данных.\n",
        "\n",
        "Когда вы строите регрессионную модель, вы выбираете функцию (например, линейную, полиномиальную или экспоненциальную), которая наилучшим образом описывает зависимость между переменными на основе имеющихся данных. Эта же функция может быть затем использована для экстраполяции, чтобы предсказать значения вне диапазона данных.\n",
        "\n",
        "Однако важно помнить, что экстраполяция может быть менее надежной, поскольку предполагает, что выявленные закономерности сохранятся за пределами изученного диапазона. Поэтому при экстраполяции стоит быть осторожным и учитывать возможные изменения в трендах или условиях."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F45bPTISDRN7"
      },
      "outputs": [],
      "source": [
        "# импортируем основные бибилиотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc3pxzM5DRN9"
      },
      "source": [
        "Загрузим данные. В практической работе можно использовать эти данные https://goo.su/PNlL или взять данные с открытых источников\n",
        "\n",
        "DataSet1_3.csv - Стоимость и объем продаж акций компании Google\n",
        "\n",
        "DataSet1_4.csv - статистика заболеваемости COVID-19 по странам\n",
        "\n",
        "DataSet3_1.csv - данные о температуре и потребляемой мощности электрической энергии\n",
        "\n",
        "DataSet3_2.csv - данные о стоимости доллара и нефти"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syDzrQYgDRN9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Стоимость и объем продаж акций компании Google (работа с временными рядами, может быть решена задача прогнозирования)\n",
        "data1 = pd.read_csv('./DataSet1_3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P95xbjGsDRN-"
      },
      "outputs": [],
      "source": [
        "# Посмотрим на содержание таблицы\n",
        "data1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvMD7Qr6DRN-"
      },
      "source": [
        "На этом семинаре мы будем строить модели только по двум переменным $x$ и $y$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tISh9HoODRN-"
      },
      "outputs": [],
      "source": [
        "# из всего объёма данных выберем 100-200 значений для прогноза (так удобнее для визуализации)\n",
        "n1 = 101\n",
        "n2 = 300\n",
        "data = pd.DataFrame()\n",
        "\n",
        "# в качестве переменной x будем использовать дни\n",
        "data1['Date'] = pd.to_datetime(data1['Date'], format=\"%Y-%m-%d\")\n",
        "data['x'] = (data1['Date'] - data1['Date'].iloc[0]).dt.days.loc[n1:n2]\n",
        "data['x'] -= data['x'].iloc[0] - 1\n",
        "#переменная, которую мы будем прогнозировать\n",
        "data['y'] = data1['Close'].loc[n1:n2]\n",
        "data.reset_index(inplace=True, drop=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkaAC8oADRN_"
      },
      "outputs": [],
      "source": [
        "#посмотрим на данные\n",
        "plt.scatter(data['x'], data['y'])\n",
        "plt.xlabel('Время')\n",
        "plt.ylabel('Объем купли/продаж')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4ItlmYUDRN_"
      },
      "source": [
        "### Метод наименьших квадратов\n",
        "\n",
        "Одним из часто встречающихся на практике методе, используемый в самых различных его модификациях, является метод наименьших квадратов (МНК). Этот метод был опубликован в современном виде **более двухсот лет назад**!\n",
        "\n",
        "Пусть у нас задан датасет $(X, y)$, где $y=(y_i)_{i=1}^N \\in \\mathbb{R}^N$ – вектор значений целевой переменной, а $X=(x_i)_{i = 1}^N \\in \\mathbb{R}^{N \\times D}, x_i \\in \\mathbb{R}^D$ – матрица объекты-признаки, в которой\n",
        "$i$-я строка – это вектор признаков $i$-го объекта выборки. Мы хотим моделировать зависимость $y_{i}$ от $x_{i}$ как линейную функцию со свободным членом. Общий вид такой функции из $\\mathbb{R}^D$  в $\\mathbb{R}$ выглядит следующим образом:\n",
        "\n",
        "$\\color{#348FEA}{f_w(x_i) = \\langle w, x_i \\rangle + w_0}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxOFezXbDRN_"
      },
      "source": [
        "**Сведение к задаче оптимизации**\n",
        "\n",
        "Мы хотим, чтобы на нашем датасете (то есть на парах $(x_i, y_i)$ из обучающей выборки) функция $f_w$ как можно лучше приближала нашу зависимость.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "(источник изображения https://academy.yandex.ru/handbook/ml/article/linear-models)\n",
        "\n",
        "Говоря простым языком, мы должны научиться измерять качество модели и минимизировать её ошибку, как-то меняя обучаемые параметры. В нашем примере обучаемые параметры — это веса $w$.\n",
        "\n",
        "Функция, оценивающая то, как часто модель ошибается, традиционно называется **функцией потерь, функционалом качества или просто лоссом (loss function)**.\n",
        "\n",
        "Функции потерь бывают разными. От их выбора зависит то, насколько задачу в дальнейшем легко решать, и то, в каком смысле у нас получится приблизить предсказание модели к целевым значениям.\n",
        "\n",
        "Интуитивно понятно, что для нашей текущей задачи нам нужно взять вектор $y$\n",
        "и вектор предсказаний модели и как-то сравнить, насколько они похожи. Так как эти вектора «живут» в одном векторном пространстве, расстояние между ними вполне может быть функцией потерь.\n",
        "\n",
        "При этом способов задать расстояние между векторами тоже довольно много. (См. лекции или Л.А.Мыльников \"Статистические методы интеллектуального анализа данных\").  Сейчас давайте в качестве лосса возьмём квадрат $L^2$-нормы вектора разницы предсказаний модели и $y$.\n",
        "\n",
        "$L(f, X, y) = |y - f(X)|_2^2 = = \\|y - Xw\\|_2^2 = \\sum_{i=1}^N(y_i - \\langle x_i, w \\rangle)^2$\n",
        "\n",
        "Однако, такой функционал ошибки не очень хорош для сравнения поведения моделей на выборках разного размера. Представьте, что вы хотите понять, насколько качество модели на тестовой выборке из 2000 объектов хуже, чем на обучающей из 7000 объектов. Вы измерили $L^2$-норму ошибки и получили в одном случае 200, а в другом 500. Эти числа не очень интерпретируемы. Гораздо лучше посмотреть на среднеквадратичное отклонение. Такая функция ошибки называется **Mean Squared Error, MSE или среднеквадратическим отклонением.**\n",
        "\n",
        "$\\color{#348FEA}{\\text{MSE}(f, X, y) =  \\frac{1}{N}|y - X w|_2^2}$\n",
        "\n",
        "Подробнее читайте здесь https://academy.yandex.ru/handbook/ml/article/linear-models\n",
        "\n",
        "Более подробно с метриками оценки моделей мы поработаем на семинаре 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6DYwB72DRN_"
      },
      "source": [
        "Итого:\n",
        "\n",
        "Для простоты попробуем для начала построить линейную функцию всего от двух переменных, приближающую наши данные $ y = ax + b$\n",
        "\n",
        "1. Используя $L^2$-норму мы имеем выражение следующего вида:\n",
        "\n",
        "$\\sum_{i=1}^N(y - y_i)^2 = \\sum_{i=1}^N(y_i - (ax_i + b))^2 \\to  min$\n",
        "\n",
        "2. Так как мы решаем задачу оптимизации, то решением её будет нахождение нулей производной. Соответственно, диффернцируем нашу формулу по $da$ и по $db$ и получаем систему из двух линейных уровнений:\n",
        "\n",
        "$\\begin{cases}\n",
        "                \\frac{\\partial f}{\\partial a} = 0\n",
        "                \\\\\n",
        "                 \\frac{\\partial f}{\\partial b} = 0\n",
        "                 \\end{cases} \\rightarrow\n",
        "\\begin{cases}\n",
        "                2\\sum_{i=1}^N (ax_i^2 + bx_i - x_iy_i)= 0\n",
        "                \\\\\n",
        "                  2\\sum_{i=1}^N (ax_i + b - y_i)= 0\n",
        "                 \\end{cases}$\n",
        "\n",
        "После упрощения:\n",
        "\n",
        "$\\begin{cases}\n",
        "                a\\sum_{i=1}^N x_i^2 + b\\sum_{i=1}^Nx_i =\\sum_{i=1}^N x_iy_i\n",
        "                \\\\\n",
        "                  a\\sum_{i=1}^N x_i + bn =\\sum_{i=1}^N y_i)\n",
        "                 \\end{cases}$\n",
        "\n",
        "\n",
        "3. Исходя из этого посчитаем угол наклона:\n",
        "\n",
        "$a = \\frac {n \\sum_{i=1}^N x_iy_i - (\\sum_{i=1}^N x_i)(\\sum_{i=1}^N y_i)}{n\\sum_{i=1}^N x_i^2 - (\\sum_{i=1}^N x_i)^2}$\n",
        "\n",
        "перейдём к средним:\n",
        "\n",
        "$a = \\frac { \\overline{xy} - \\bar{x}\\bar{y}}{\\frac{\\sum_{i=1}^N x_i^2}{n} - (\\bar{x})^2}$\n",
        "\n",
        "4. Посчитаем пересечение с осью y:\n",
        "\n",
        "$b= \\frac {\\sum_{i=1}^N y_i - a \\sum_{i=1}^N x_i} {n}$.\n",
        "\n",
        "Заметим, что это средние значения по $x$ и по $y$. Соответственно,\n",
        "\n",
        "$b = \\bar {y} - a \\bar{x}$\n",
        "\n",
        "Вот мы и получили уравнение прямой!\n",
        "\n",
        "Для построения более сложных функций (гиперболы, параболы и др.) порядок построения МНК точно такой же, главное аккуратно взять частные производные от используемых функций.\n",
        "\n",
        "Теперь реализуем вычисления на python и посмотрим на результат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABT0DdyODRN_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data['x']\n",
        "Y = data['y']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=False)\n",
        "# в этой задаче отключим перемешивание данных в трейне и тесте, так как мы концентрируемся на экстраполяции!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u37tbGTIDRN_"
      },
      "outputs": [],
      "source": [
        "def MNK(X, Y):\n",
        "    # TODO: реализовать вычисление коэффициентов МНК самостоятельно\n",
        "\n",
        "    return a, b\n",
        "\n",
        "a, b = MNK(X_train, y_train)\n",
        "\n",
        "# Выводим параметры регрессии\n",
        "print(f'Уравнение регрессии: y = {b:.2f} + {a:.8f}x')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeH7O6soDROA"
      },
      "outputs": [],
      "source": [
        "# добавим предсказание по тестовой выборке\n",
        "y_pred = b + a * X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhVnSB7zDROA"
      },
      "outputs": [],
      "source": [
        "# Визуализируем данные и регрессионную линию\n",
        "plt.scatter(X_train, y_train, label='Исходные данные')\n",
        "plt.plot(X_train, a*X_train+b, color='green', label='МНК', ls='dashed')\n",
        "plt.scatter(X_test, y_test, color='orange', label='Тестовые данные')\n",
        "plt.plot(X_test, y_pred, color='red', label='ЛР на тестовых данных')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0K0Lu3dDROA"
      },
      "source": [
        "Реализация с помощью sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDda5S0RDROA"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "x_train = np.array(X_train).reshape(-1, 1)\n",
        "x_test = np.array(X_test).reshape(-1, 1)\n",
        "model.fit(x_train, y_train)\n",
        "res_MNK = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2Xzf3OlDROA"
      },
      "outputs": [],
      "source": [
        "# Визуализируем данные и регрессионную линию\n",
        "plt.scatter(x_train, y_train, label='Исходные данные')\n",
        "plt.plot(x_train, model.predict(x_train) , color='green', label='ЛР на тренировочных данных', ls='dashed')\n",
        "plt.scatter(x_test, y_test, color='orange', label='Тестовые данные')\n",
        "plt.plot(x_test, res_MNK, color='red', label='ЛР на тестовых данных')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2EC1KCGDROA"
      },
      "source": [
        "МНК для параболы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuwQpPqTDROA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# Подготовка данных для модели\n",
        "X1_train = np.column_stack((np.ones(X_train.shape[0]), X_train, X_train**2))  # Добавляем столбец единиц для c0\n",
        "X1_test = np.column_stack((np.ones(X_test.shape[0]), X_test, X_test**2))\n",
        "# Создание и обучение модели\n",
        "model = LinearRegression()\n",
        "model.fit(X1_train, y_train)\n",
        "\n",
        "# Получение коэффициентов\n",
        "c0, c1, c2 = model.intercept_, model.coef_[1], model.coef_[2]\n",
        "print(f'Коэффициенты: c0 = {c0}, c1 = {c1}, c2 = {c2}')\n",
        "\n",
        "# Построение графика\n",
        "plt.scatter(x_train, y_train, label='Исходные данные')  # Исходные данные\n",
        "plt.plot(x_train, model.predict(X1_train), label='Модель', color='green', ls='dashed')  # Предсказанные значения\n",
        "\n",
        "plt.scatter(x_test, y_test, color='orange', label='Тестовые данные')\n",
        "plt.plot(x_test, model.predict(X1_test), color='red', label='Модель на тестовых данных')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title(r'МНК для функции $y = c_0 + c_1 \\cdot x + c_2 \\cdot x^2$ ')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuC56-goDROB"
      },
      "outputs": [],
      "source": [
        "print('intercept:', model.intercept_)\n",
        "print('slope:', model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_aGXO22DROB"
      },
      "source": [
        "**NOTE:** В общем случае, аналитическое решение задачи с помощью МНК очень трудозатратно. В случае, если мы решаем задачу оптимизации от достаточно большого количества переменных (их могут быть сотни), при вычислении результирующего вектора коэффициентов возникает необходимость вычисления обратной матрицы. Это очень трудозатратный процесс. Поэтому, обычно эту задачу решают численно, используя метод градиентного спуска."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejWvN1xXDROB"
      },
      "source": [
        "### Метод К-ближайших соседей (K-neariest neighbor, kNN)\n",
        "\n",
        "Метод kNN относится к метрическим методам.\n",
        "Смысл метрических методов очень хорошо раскрывает фраза *«Скажи мне, кто твой друг, и я скажу, кто ты»*. Алгоритмы этого класса почти не имеют фазы обучения. Вместо этого они просто запоминают всю обучающую выборку, а на этапе предсказания просто ищут похожие на целевой объекты. Такой процесс называют lazy learning, потому что никакого обучения, по сути, не происходит.\n",
        "\n",
        "Метод основан на предположении о том, что близким объектам в признаковом пространстве соответствуют похожие метки.\n",
        "\n",
        "Для нового объекта $x$ метод предполагает найти ближайшие к нему объекты $x_{1},x_{2},...x_{K}$ и построить прогноз по их меткам.\n",
        "\n",
        "В случае построения регрессионной модели, используем формулу:\n",
        "\n",
        "$ \\hat{y} = \\frac{\\sum_{i=1}^K y_i}{K} $\n",
        "\n",
        "K - это гипер-параметр и вообще должен подбираться с помощью кросс-валидации.\n",
        "\n",
        "Очень хорошо про KNN рассказано вот здесь https://dlcourse.ai/. Рекомендовано к изучению!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XE0IRukpDROB"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "x = np.array(X).reshape(-1, 1)\n",
        "# Разделите данные на обучающий и тестовый наборы\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, Y, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "# Создайте модель k-NN регрессии\n",
        "k = 7  # Задайте количество соседей\n",
        "model = KNeighborsRegressor(n_neighbors=k)\n",
        "\n",
        "# Обучите модель на обучающих данных\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Получите предсказания для тестового набора\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Визуализация данных и регрессии\n",
        "plt.scatter(x_train, y_train, label='Тренировочные данные данные', color='black')\n",
        "plt.scatter(x_test, y_test, label='Тестовые данные', color='blue')\n",
        "plt.plot(x_test, y_pred, label=f'k-NN регрессия (k={k})', color='red')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCkhS_Z1DROB"
      },
      "source": [
        "# Модель авторегрессии скользящего среднего (autoregressive integrated moving average, ARIMA)\n",
        "\n",
        " Она предназначена для анализа стационарных временных рядов на основе оценки линейной зависимости прогнозируемых значений от исторических.\n",
        "\n",
        "Для использования модели временной ряд должен быть стационарным, т.е. его среднее и дисперсия должны быть постоянны.\n",
        "\n",
        "Модель Бокса-Дженкинса (это метод анализа временных рядов, который используется для построения моделей, способных описывать и прогнозировать временные данные) предполагает, что временной ряд содержит три составляющие: авторегресионную, интегрированную и скользящее среднее, которые в модели обозначены $p, d$ и $q$ соответственно:\n",
        "\n",
        "- Величина $p$ называется порядком авторегрессии. Она позволяет ответить на вопрос, будет ли очередной элемент ряда близок к значению\n",
        "X, если к нему были близки $p$ предыдущих значений.\n",
        "\n",
        "- Величину $d$ называют порядком интегрирования. Она показывает, насколько элемент ряда близок по значению к $d$ предыдущим значениям, если разность между ними минимальна.\n",
        "\n",
        "- Параметр $q$ — порядок скользящего среднего. Позволяет установить погрешность модели как линейную комбинацию наблюдавшихся ранее значений ошибок.\n",
        "\n",
        "Авторегрессия — это составляющая модели временного ряда, в которой его прогнозируемое значение может быть выражено в виде линейной комбинации исторических значений этого же ряда и случайной ошибки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_F3-IMYDROB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "x = data['x']\n",
        "y = data['y']\n",
        "# Разделите данные на обучающий и тестовый наборы\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Постройте модель ARIMA для временного ряда y\n",
        "model_y = ARIMA(y_train, order=(3, 2, 1))\n",
        "results_y = model_y.fit()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Прогноз для y\n",
        "forecast_y = results_y.forecast(steps=len(y_test))\n",
        "\n",
        "# Визуализируйте результаты прогноза\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(x_train, y_train, label='y (исходные данные)')\n",
        "plt.scatter(x_test, y_test, color='red')\n",
        "plt.plot(x_test, forecast_y, label='Прогноз y', color='green')\n",
        "plt.xlabel('Время')\n",
        "plt.ylabel('Значения')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GduFGEI-DROB"
      },
      "outputs": [],
      "source": [
        "print(\"\\nСтатистика для y:\")\n",
        "print(results_y.summary())\n",
        "results_y.plot_diagnostics(figsize=(10, 8))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MNE_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
